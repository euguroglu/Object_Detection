{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detecting Objects in Real Time with OpenCV deep learning library\n",
    "\n",
    "Algorithm:\n",
    "Reading stream video from camera --> Loading YOLO v3 Network -->\n",
    "--> Reading frames in the loop --> Getting blob from the frame -->\n",
    "--> Implementing Forward Pass --> Getting Bounding Boxes -->\n",
    "--> Non-maximum Suppression --> Drawing Bounding Boxes with Labels -->\n",
    "--> Showing processed frames in OpenCV Window\n",
    "\n",
    "Result:\n",
    "Window with Detected Objects, Bounding Boxes and Labels in Real Time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current frame took 0.69186 seconds\n",
      "Current frame took 0.52160 seconds\n",
      "Current frame took 0.50066 seconds\n",
      "Current frame took 0.50859 seconds\n",
      "Current frame took 0.49068 seconds\n",
      "Current frame took 0.47772 seconds\n",
      "Current frame took 0.48770 seconds\n",
      "Current frame took 0.49467 seconds\n",
      "Current frame took 0.50465 seconds\n",
      "Current frame took 0.46675 seconds\n",
      "Current frame took 0.48171 seconds\n",
      "Current frame took 0.49367 seconds\n",
      "Current frame took 0.46576 seconds\n",
      "Current frame took 0.50465 seconds\n",
      "Current frame took 0.50564 seconds\n",
      "Current frame took 0.47174 seconds\n",
      "Current frame took 0.49265 seconds\n",
      "Current frame took 0.47672 seconds\n",
      "Current frame took 0.47273 seconds\n",
      "Current frame took 0.47972 seconds\n",
      "Current frame took 0.46676 seconds\n",
      "Current frame took 0.49663 seconds\n",
      "Current frame took 0.47971 seconds\n",
      "Current frame took 0.57346 seconds\n",
      "Current frame took 0.51160 seconds\n",
      "Current frame took 0.60239 seconds\n",
      "Current frame took 0.55336 seconds\n",
      "Current frame took 0.49664 seconds\n",
      "Current frame took 0.46675 seconds\n",
      "Current frame took 0.47174 seconds\n",
      "Current frame took 0.47171 seconds\n",
      "Current frame took 0.46376 seconds\n",
      "Current frame took 0.49367 seconds\n",
      "Current frame took 0.48566 seconds\n",
      "Current frame took 0.47772 seconds\n",
      "Current frame took 0.47173 seconds\n",
      "Current frame took 0.48570 seconds\n",
      "Current frame took 0.49667 seconds\n",
      "Current frame took 0.47572 seconds\n",
      "Current frame took 0.47872 seconds\n",
      "Current frame took 0.49468 seconds\n",
      "Current frame took 0.49168 seconds\n",
      "Current frame took 0.46276 seconds\n",
      "Current frame took 0.45578 seconds\n",
      "Current frame took 0.46276 seconds\n",
      "Current frame took 0.47572 seconds\n",
      "Current frame took 0.46676 seconds\n",
      "Current frame took 0.46875 seconds\n",
      "Current frame took 0.47170 seconds\n",
      "Current frame took 0.46875 seconds\n",
      "Current frame took 0.47673 seconds\n",
      "Current frame took 0.45977 seconds\n",
      "Current frame took 0.47074 seconds\n",
      "Current frame took 0.51961 seconds\n",
      "Current frame took 0.56848 seconds\n",
      "Current frame took 0.53158 seconds\n",
      "Current frame took 0.51163 seconds\n",
      "Current frame took 0.49069 seconds\n",
      "Current frame took 0.46077 seconds\n",
      "Current frame took 0.47968 seconds\n",
      "Current frame took 0.45977 seconds\n",
      "Current frame took 0.47074 seconds\n",
      "Current frame took 0.49168 seconds\n",
      "Current frame took 0.46974 seconds\n",
      "Current frame took 0.46974 seconds\n",
      "Current frame took 0.47573 seconds\n",
      "Current frame took 0.46273 seconds\n",
      "Current frame took 0.45874 seconds\n",
      "Current frame took 0.47373 seconds\n",
      "Current frame took 0.48769 seconds\n",
      "Current frame took 0.45977 seconds\n",
      "Current frame took 0.48667 seconds\n",
      "Current frame took 0.47274 seconds\n",
      "Current frame took 0.46077 seconds\n",
      "Current frame took 0.47370 seconds\n",
      "Current frame took 0.46775 seconds\n",
      "Current frame took 0.50564 seconds\n",
      "Current frame took 0.47571 seconds\n",
      "Current frame took 0.46974 seconds\n",
      "Current frame took 0.48370 seconds\n",
      "Current frame took 0.46574 seconds\n",
      "Current frame took 0.49467 seconds\n",
      "Current frame took 0.51861 seconds\n",
      "Current frame took 0.57546 seconds\n",
      "Current frame took 0.54155 seconds\n",
      "Current frame took 0.52460 seconds\n",
      "Current frame took 0.46376 seconds\n",
      "Current frame took 0.49069 seconds\n",
      "Current frame took 0.48670 seconds\n",
      "Current frame took 0.49168 seconds\n",
      "Current frame took 0.47573 seconds\n",
      "Current frame took 0.47672 seconds\n",
      "Current frame took 0.47772 seconds\n",
      "Current frame took 0.46177 seconds\n",
      "Current frame took 0.48370 seconds\n",
      "Current frame took 0.46974 seconds\n",
      "Current frame took 0.47872 seconds\n",
      "Current frame took 0.47171 seconds\n",
      "Current frame took 0.50365 seconds\n",
      "Current frame took 0.47474 seconds\n",
      "Current frame took 0.46675 seconds\n",
      "Current frame took 0.50066 seconds\n",
      "Current frame took 0.46176 seconds\n",
      "Current frame took 0.47074 seconds\n",
      "Current frame took 0.50465 seconds\n",
      "Current frame took 0.46476 seconds\n",
      "Current frame took 0.47074 seconds\n",
      "Current frame took 0.51163 seconds\n",
      "Current frame took 0.49667 seconds\n",
      "Current frame took 0.49863 seconds\n",
      "Current frame took 0.49168 seconds\n",
      "Current frame took 0.50365 seconds\n",
      "Current frame took 0.59242 seconds\n",
      "Current frame took 0.52311 seconds\n",
      "Current frame took 0.53457 seconds\n",
      "Current frame took 0.47469 seconds\n",
      "Current frame took 0.49467 seconds\n",
      "Current frame took 0.49268 seconds\n",
      "Current frame took 0.47470 seconds\n",
      "Current frame took 0.46276 seconds\n",
      "Current frame took 0.49464 seconds\n",
      "Current frame took 0.47372 seconds\n",
      "Current frame took 0.46276 seconds\n",
      "Current frame took 0.48371 seconds\n",
      "Current frame took 0.47373 seconds\n",
      "Current frame took 0.46575 seconds\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import time\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Start of:\n",
    "Reading stream video from camera\n",
    "\"\"\"\n",
    "\n",
    "# Defining 'VideoCapture' object\n",
    "# and reading stream video from camera\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "# Preparing variables for spatial dimensions of the frames\n",
    "h, w = None, None\n",
    "\n",
    "\"\"\"\n",
    "End of:\n",
    "Reading stream video from camera\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Start of:\n",
    "Loading YOLO v3 network\n",
    "\"\"\"\n",
    "\n",
    "# Loading COCO class labels from file\n",
    "# Opening file\n",
    "\n",
    "with open('yolo-coco-data/coco.names') as f:\n",
    "    # Getting labels reading every line\n",
    "    # and putting them into the list\n",
    "    labels = [line.strip() for line in f]\n",
    "\n",
    "\n",
    "# # Check point\n",
    "# print('List with labels names:')\n",
    "# print(labels)\n",
    "\n",
    "# Loading trained YOLO v3 Objects Detector\n",
    "# with the help of 'dnn' library from OpenCV\n",
    "\n",
    "network = cv2.dnn.readNetFromDarknet('yolo-coco-data/yolov4.cfg',\n",
    "                                     'yolo-coco-data/yolov4.weights')\n",
    "\n",
    "# Getting list with names of all layers from YOLO v3 network\n",
    "layers_names_all = network.getLayerNames()\n",
    "\n",
    "# # Check point\n",
    "# print()\n",
    "# print(layers_names_all)\n",
    "\n",
    "# Getting only output layers' names that we need from YOLO v3 algorithm\n",
    "# with function that returns indexes of layers with unconnected outputs\n",
    "layers_names_output = [layers_names_all[i[0] - 1] for i in network.getUnconnectedOutLayers()]\n",
    "\n",
    "# # Check point\n",
    "# print()\n",
    "# print(layers_names_output)  # ['yolo_82', 'yolo_94', 'yolo_106']\n",
    "\n",
    "# Setting minimum probability to eliminate weak predictions\n",
    "probability_minimum = 0.5\n",
    "\n",
    "# Setting threshold for filtering weak bounding boxes\n",
    "# with non-maximum suppression\n",
    "threshold = 0.3\n",
    "\n",
    "# Generating colours for representing every detected object\n",
    "# with function randint(low, high=None, size=None, dtype='l')\n",
    "colours = np.random.randint(0, 255, size=(len(labels), 3), dtype='uint8')\n",
    "\n",
    "# # Check point\n",
    "# print()\n",
    "# print(type(colours))  # <class 'numpy.ndarray'>\n",
    "# print(colours.shape)  # (80, 3)\n",
    "# print(colours[0])  # [172  10 127]\n",
    "\n",
    "\"\"\"\n",
    "End of:\n",
    "Loading YOLO v3 network\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Start of:\n",
    "Reading frames in the loop\n",
    "\"\"\"\n",
    "\n",
    "# Defining loop for catching frames\n",
    "while True:\n",
    "    # Capturing frame-by-frame from camera\n",
    "    _, frame = camera.read()\n",
    "\n",
    "    # Getting spatial dimensions of the frame\n",
    "    # we do it only once from the very beginning\n",
    "    # all other frames have the same dimension\n",
    "    if w is None or h is None:\n",
    "        # Slicing from tuple only first two elements\n",
    "        h, w = frame.shape[:2]\n",
    "\n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Getting blob from current frame\n",
    "    \"\"\"\n",
    "\n",
    "    # Getting blob from current frame\n",
    "    # The 'cv2.dnn.blobFromImage' function returns 4-dimensional blob from current\n",
    "    # frame after mean subtraction, normalizing, and RB channels swapping\n",
    "    # Resulted shape has number of frames, number of channels, width and height\n",
    "    # E.G.:\n",
    "    # blob = cv2.dnn.blobFromImage(image, scalefactor=1.0, size, mean, swapRB=True)\n",
    "    blob = cv2.dnn.blobFromImage(frame, 1 / 255.0, (416, 416),\n",
    "                                 swapRB=True, crop=False)\n",
    "\n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Getting blob from current frame\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Implementing Forward pass\n",
    "    \"\"\"\n",
    "\n",
    "    # Implementing forward pass with our blob and only through output layers\n",
    "    # Calculating at the same time, needed time for forward pass\n",
    "    network.setInput(blob)  # setting blob as input to the network\n",
    "    start = time.time()\n",
    "    output_from_network = network.forward(layers_names_output)\n",
    "    end = time.time()\n",
    "\n",
    "    # Showing spent time for single current frame\n",
    "    print('Current frame took {:.5f} seconds'.format(end - start))\n",
    "\n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Implementing Forward pass\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Getting bounding boxes\n",
    "    \"\"\"\n",
    "\n",
    "    # Preparing lists for detected bounding boxes,\n",
    "    # obtained confidences and class's number\n",
    "    bounding_boxes = []\n",
    "    confidences = []\n",
    "    class_numbers = []\n",
    "\n",
    "    # Going through all output layers after feed forward pass\n",
    "    for result in output_from_network:\n",
    "        # Going through all detections from current output layer\n",
    "        for detected_objects in result:\n",
    "            # Getting 80 classes' probabilities for current detected object\n",
    "            scores = detected_objects[5:]\n",
    "            # Getting index of the class with the maximum value of probability\n",
    "            class_current = np.argmax(scores)\n",
    "            # Getting value of probability for defined class\n",
    "            confidence_current = scores[class_current]\n",
    "\n",
    "            # # Check point\n",
    "            # # Every 'detected_objects' numpy array has first 4 numbers with\n",
    "            # # bounding box coordinates and rest 80 with probabilities\n",
    "            # # for every class\n",
    "            # print(detected_objects.shape)  # (85,)\n",
    "\n",
    "            # Eliminating weak predictions with minimum probability\n",
    "            if confidence_current > probability_minimum:\n",
    "                # Scaling bounding box coordinates to the initial frame size\n",
    "                # YOLO data format keeps coordinates for center of bounding box\n",
    "                # and its current width and height\n",
    "                # That is why we can just multiply them elementwise\n",
    "                # to the width and height\n",
    "                # of the original frame and in this way get coordinates for center\n",
    "                # of bounding box, its width and height for original frame\n",
    "                box_current = detected_objects[0:4] * np.array([w, h, w, h])\n",
    "\n",
    "                # Now, from YOLO data format, we can get top left corner coordinates\n",
    "                # that are x_min and y_min\n",
    "                x_center, y_center, box_width, box_height = box_current\n",
    "                x_min = int(x_center - (box_width / 2))\n",
    "                y_min = int(y_center - (box_height / 2))\n",
    "\n",
    "                # Adding results into prepared lists\n",
    "                bounding_boxes.append([x_min, y_min,\n",
    "                                       int(box_width), int(box_height)])\n",
    "                confidences.append(float(confidence_current))\n",
    "                class_numbers.append(class_current)\n",
    "\n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Getting bounding boxes\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Non-maximum suppression\n",
    "    \"\"\"\n",
    "\n",
    "    # Implementing non-maximum suppression of given bounding boxes\n",
    "    # With this technique we exclude some of bounding boxes if their\n",
    "    # corresponding confidences are low or there is another\n",
    "    # bounding box for this region with higher confidence\n",
    "\n",
    "    # It is needed to make sure that data type of the boxes is 'int'\n",
    "    # and data type of the confidences is 'float'\n",
    "    # https://github.com/opencv/opencv/issues/12789\n",
    "    results = cv2.dnn.NMSBoxes(bounding_boxes, confidences,\n",
    "                               probability_minimum, threshold)\n",
    "\n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Non-maximum suppression\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Drawing bounding boxes and labels\n",
    "    \"\"\"\n",
    "\n",
    "    # Checking if there is at least one detected object\n",
    "    # after non-maximum suppression\n",
    "    if len(results) > 0:\n",
    "        # Going through indexes of results\n",
    "        for i in results.flatten():\n",
    "            # Getting current bounding box coordinates,\n",
    "            # its width and height\n",
    "            x_min, y_min = bounding_boxes[i][0], bounding_boxes[i][1]\n",
    "            box_width, box_height = bounding_boxes[i][2], bounding_boxes[i][3]\n",
    "\n",
    "            # Preparing colour for current bounding box\n",
    "            # and converting from numpy array to list\n",
    "            colour_box_current = colours[class_numbers[i]].tolist()\n",
    "\n",
    "            # # # Check point\n",
    "            # print(type(colour_box_current))  # <class 'list'>\n",
    "            # print(colour_box_current)  # [172 , 10, 127]\n",
    "\n",
    "            # Drawing bounding box on the original current frame\n",
    "            cv2.rectangle(frame, (x_min, y_min),\n",
    "                          (x_min + box_width, y_min + box_height),\n",
    "                          colour_box_current, 2)\n",
    "\n",
    "            # Preparing text with label and confidence for current bounding box\n",
    "            text_box_current = '{}: {:.4f}'.format(labels[int(class_numbers[i])],\n",
    "                                                   confidences[i])\n",
    "\n",
    "            # Putting text with label and confidence on the original image\n",
    "            cv2.putText(frame, text_box_current, (x_min, y_min - 5),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 0.5, colour_box_current, 2)\n",
    "\n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Drawing bounding boxes and labels\n",
    "    \"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Start of:\n",
    "    Showing processed frames in OpenCV Window\n",
    "    \"\"\"\n",
    "\n",
    "    # Showing results obtained from camera in Real Time\n",
    "\n",
    "    # Showing current frame with detected objects\n",
    "    # Giving name to the window with current frame\n",
    "    # And specifying that window is resizable\n",
    "    cv2.namedWindow('YOLO v3 Real Time Detections', cv2.WINDOW_NORMAL)\n",
    "    # Pay attention! 'cv2.imshow' takes images in BGR format\n",
    "    cv2.imshow('YOLO v3 Real Time Detections', frame)\n",
    "\n",
    "    # Breaking the loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "    \"\"\"\n",
    "    End of:\n",
    "    Showing processed frames in OpenCV Window\n",
    "    \"\"\"\n",
    "\n",
    "\"\"\"\n",
    "End of:\n",
    "Reading frames in the loop\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Releasing camera\n",
    "camera.release()\n",
    "# Destroying all opened OpenCV windows\n",
    "cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Some comments\n",
    "\n",
    "cv2.VideoCapture(0)\n",
    "\n",
    "To capture video, it is needed to create VideoCapture object.\n",
    "Its argument can be camera's index or name of video file.\n",
    "Camera index is usually 0 for built-in one.\n",
    "Try to select other cameras by passing 1, 2, 3, etc.\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
